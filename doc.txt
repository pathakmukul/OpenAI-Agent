# Building an LLM Agent Tool for Automated Multi-Step Tasks

This documentation provides a comprehensive guide on how to build a Language Model (LLM) Agent tool capable of automating complex tasks involving multiple steps. The tool leverages OpenAI's GPT models to plan and execute tasks by coordinating multiple specialized agents. The key components include the **Master Agent**, various specialized **Agents** (e.g., Image Agent, Code Agent, File Agent), and the underlying infrastructure that supports planning, context management, and execution.

## Table of Contents

1. [Overview](#overview)
2. [Architecture](#architecture)
   - [Master Agent](#master-agent)
   - [Specialized Agents](#specialized-agents)
3. [Components](#components)
   - [Planning and Execution Flow](#planning-and-execution-flow)
   - [Context Management](#context-management)
   - [Assistant Creation and Usage](#assistant-creation-and-usage)
   - [Thread Management](#thread-management)
4. [Agents Structure](#agents-structure)
   - [Image Agent](#image-agent)
   - [Code Agent](#code-agent)
   - [File Agent](#file-agent)
5. [Implementing Complex Tasks Automation](#implementing-complex-tasks-automation)
6. [JSON Planning and Responses](#json-planning-and-responses)
7. [Error Handling and Logging](#error-handling-and-logging)
8. [Security Considerations](#security-considerations)
9. [Conclusion](#conclusion)
10. [Appendix](#appendix)
    - [Sample Code Snippets](#sample-code-snippets)

---

## Overview

The LLM Agent tool is designed to automate complex tasks by decomposing them into smaller, manageable steps and delegating these steps to specialized agents. The **Master Agent** orchestrates this process by:

1. Receiving a user task.
2. Creating a plan to complete the task.
3. Invoking specialized agents according to the plan.
4. Managing context and dependencies between agents.
5. Combining the results and presenting them to the user.

---

## Architecture

### Master Agent

The **Master Agent** serves as the central coordinator. Its primary responsibilities include:

- **Task Planning**: Uses an LLM to generate a detailed plan in JSON format, specifying which agents to use and the prompts for each.
- **Plan Parsing**: Parses the JSON plan and extracts the sequence of agent invocations.
- **Agent Invocation**: Calls the appropriate agents with the specified prompts.
- **Context Management**: Maintains and passes context between agents to handle dependencies.
- **Result Aggregation**: Collects results from agents and presents a consolidated output to the user.

### Specialized Agents

Specialized agents perform specific tasks:

- **Image Agent**: Generates images based on textual descriptions.
- **Code Agent**: Writes and executes code.
- **File Agent**: Manages file operations like reading, writing, and downloading files.

Each agent operates independently but can receive context from previous agents to fulfill their tasks.

---

## Components

### Planning and Execution Flow

1. **Receive User Task**: The Master Agent receives a complex task described in natural language.

2. **Create Planning Prompt**: It formulates a planning prompt that includes:

   - The user task.
   - Instructions for planning.
   - Definitions of available agents.
   - Rules for creating the plan.
   - A JSON structure for the plan.

3. **Generate Plan**: The Master Agent sends the planning prompt to the LLM to generate a plan in JSON format.

4. **Parse Plan**: It parses the JSON plan, ensuring it contains the required structure.

5. **Execute Plan**: For each step in the plan:

   - Identifies the agent to invoke.
   - Prepares the agent prompt, including any necessary context.
   - Calls the agent and collects the result.
   - Updates the context with new information from the agent.

6. **Aggregate Results**: After executing all steps, the Master Agent aggregates the results from all agents.

7. **Present Output**: The final output is presented to the user, often with enhanced formatting for readability.

### Context Management

Context management is crucial for handling dependencies between agents. The Master Agent:

- Maintains a **context dictionary** that stores results from agents.
- Passes relevant context to agents when invoking them.
- Updates the context after each agent execution.

### Assistant Creation and Usage

Agents use OpenAI's API to create assistants with specific instructions and capabilities:

- **Instructions**: Define the assistant's role and behavior.
- **Model Selection**: Choose the appropriate GPT model (e.g., `gpt-4`, `gpt-4-32k`).
- **Tools**: Specify any tools or functions the assistant can use, such as code execution or function calls.
- **Thread Management**: Interact with assistants through threads and runs to manage conversations and actions.

### Thread Management

- **Threads**: Represent a conversation with an assistant. Each thread is associated with a unique ID.
- **Runs**: Actions taken within a thread. A run may involve the assistant generating a response, executing code, or calling a function.
- **Status Checking**: The agents poll the run status to determine when a response is ready or if action is required.
- **Tool Outputs**: If the assistant requires action (e.g., function execution), the agent handles it and submits the output back to the assistant.

---

## Agents Structure

Each specialized agent follows a similar structure but is tailored to its specific functionality.

### Image Agent

- **Purpose**: Generates images based on textual prompts.
- **Implementation**:
  - Uses a function `generate_image` to interface with an image generation API (e.g., Replicate API).
  - The assistant is created with the capability to call this function.
- **Process**:
  1. Create an assistant with instructions and the `generate_image` function.
  2. Start a thread and send the user's prompt.
  3. Create a run and wait for the assistant's response.
  4. If the assistant calls `generate_image`, execute it and submit the output.
  5. Retrieve the final image URL from the assistant's response.

### Code Agent

- **Purpose**: Writes and optionally executes code based on prompts.
- **Implementation**:
  - Creates an assistant with instructions emphasizing code generation.
  - Can include a code interpreter tool if code execution is needed.
- **Process**:
  1. Create an assistant with instructions and tools (if execution is required).
  2. Start a thread and send the prompt.
  3. Create a run and wait for the assistant's response.
  4. Retrieve the code from the assistant's message content.
  5. Optionally execute the code and collect the output.

### File Agent

- **Purpose**: Manages file operations such as reading, writing, and downloading.
- **Implementation**:
  - Provides functions like `read_file`, `write_file`, and `download_image`.
  - The assistant is created with these functions as callable tools.
- **Process**:
  1. Create an assistant with instructions and the file operation functions.
  2. Start a thread and send the prompt.
  3. Create a run and wait for the assistant's response.
  4. If the assistant calls any file functions, execute them and submit the outputs.
  5. Retrieve the assistant's final response.

---

## Implementing Complex Tasks Automation

To automate complex tasks with multiple steps:

1. **Task Decomposition**: The Master Agent decomposes the user task into smaller, manageable steps by generating a plan.

2. **Planning with LLM**: Uses an LLM to generate a detailed plan in JSON format, specifying which agents to use and their prompts.

   - **Planning Prompt**: Includes clear instructions, definitions of agents, rules, and a required JSON structure.
   - **Example**:

     ```json
     {
       "plan": [
         { "agent": "image", "prompt": "Generate an image of a sunset over mountains." },
         { "agent": "code", "prompt": "Write HTML code to display the image." },
         { "agent": "file", "prompt": "Save the HTML code as index.html in the current directory." }
       ]
     }
     ```

3. **Sequential Execution**: The Master Agent executes the plan step by step, invoking the appropriate agents.

4. **Context Passing**: Shares context between agents to handle dependencies (e.g., passing the image URL from the Image Agent to the Code Agent).

5. **Result Aggregation**: Collects and combines results from all agents to produce the final output.

---

## JSON Planning and Responses

- **Planning Output**: The LLM generates a plan in JSON format, which the Master Agent parses and executes.
- **JSON Structure**:

  ```json
  {
    "plan": [
      { "agent": "agent_name", "prompt": "specific_action_to_perform" }
    ]
  }
  ```

- **Parsing and Validation**: The Master Agent ensures the JSON is valid and contains the necessary keys.

- **Error Handling**: If parsing fails, the Master Agent reports the error and halts execution.

---

## Error Handling and Logging

- **Error Detection**: The agents detect errors at each step, such as invalid JSON, failed API calls, or exceptions during execution.

- **Logging**: The tool logs information at various levels (info, warning, error) to assist in debugging and monitoring.

- **User Feedback**: Errors are reported back to the user through the console output, using formatted messages for clarity.

---

## Security Considerations

- **API Keys**: Store API keys securely using environment variables (e.g., `.env` file) and avoid hardcoding them in the code.

- **Sensitive Data**: Be cautious when handling sensitive data, ensuring compliance with data protection regulations.

- **Function Execution**: When executing code or functions, ensure proper validation and sandboxing to prevent malicious code execution.

---

## Conclusion

By structuring the tool with a Master Agent and specialized agents, complex tasks can be automated effectively. The use of an LLM for planning and execution allows for flexibility and adaptability in handling various tasks. Context management and careful design of agent interactions are key to successfully automating multi-step processes.

---

## Appendix

### Sample Code Snippets

Below are simplified examples to illustrate key components.

#### Master Agent Planning Prompt

```python
planning_prompt = (
    f"<task>{user_task}</task>\n\n"
    f"<instructions>"
    f"As the Master Agent, your task is to make a clear and concise plan using the following agents:\n"
    f"<agents>"
    f"<agent name='image'>Image Generation Agent: Generates images based on text descriptions.</agent>\n"
    f"<agent name='code'>Code Generation Agent: Writes and executes programming code.</agent>\n"
    f"<agent name='file'>File Management Agent: Reads and writes files in the current directory.</agent>\n"
    f"</agents>\n"
    f"Create a simple plan using the agents as follows:\n"
    f"1. Assign specific tasks to each agent.\n"
    f"2. Provide only one prompt for each agent, specifying the exact action they need to perform.\n"
    f"3. Output the plan in valid JSON format, as shown below:\n"
    f"<json_structure>\n"
    f"{{\n"
    f"  \"plan\": [\n"
    f"    {{ \"agent\": \"agent_name\", \"prompt\": \"specific_action_to_perform\" }}\n"
    f"  ]\n"
    f"}}\n"
    f"</json_structure>\n"
    f"<rules>\n"
    f"- Only use each agent once unless necessary.\n"
    f"- Keep instructions simple and direct. Avoid multi-step tasks in a single prompt.\n"
    f"- Make sure the JSON is properly formatted without extra text or code blocks.\n"
    f"</rules>\n"
    f"</instructions>"
)
```

#### Agent Invocation Loop

```python
for step in plan['plan']:
    agent_name = step['agent']
    agent_prompt = step['prompt']
    console.print(f"[info]Invoking {agent_name.capitalize()} Agent...[/info]")
    
    # Include context in agent prompts
    if context:
        agent_prompt = f"{agent_prompt}\n\nContext from previous agents:\n{json.dumps(context, indent=2)}"
    
    if agent_name.lower() == 'image':
        # Invoke Image Agent
        image_result = run_image_agent(agent_prompt, client)
        results['image'] = image_result
        context['image'] = image_result
    elif agent_name.lower() == 'code':
        # Invoke Code Agent
        code_result = run_code_agent(agent_prompt, execute_code=False)
        results['code'] = code_result
        context['code'] = code_result
    elif agent_name.lower() == 'file':
        # Invoke File Agent
        file_result = run_file_agent(agent_prompt)
        results['file'] = file_result
        context['file_result'] = file_result
    else:
        console.print(f"[warning]Unknown agent: {agent_name}[/warning]")
```

#### Assistant Creation (Example from Code Agent)

```python
def run_assistant(prompt, execute_code=False):
    client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
    
    assistant_instructions = (
        "You are an expert programmer. Use all provided information, such as image URLs or other data, to write code that accomplishes the task."
    )
    
    tools = [{"type": "code_interpreter"}] if execute_code else []
    
    assistant = client.beta.assistants.create(
        name="Code Generator",
        instructions=assistant_instructions,
        tools=tools,
        model="gpt-4"
    )
    
    thread = client.beta.threads.create()
    
    client.beta.threads.messages.create(
        thread_id=thread.id,
        role="user",
        content=prompt
    )
    
    run = client.beta.threads.runs.create(
        thread_id=thread.id,
        assistant_id=assistant.id
    )
    
    # Wait for completion and retrieve response...
```

---

By following this structure and understanding the interplay between the Master Agent, specialized agents, context management, and assistant interactions, you can build an LLM Agent tool capable of automating complex, multi-step tasks efficiently and effectively.
